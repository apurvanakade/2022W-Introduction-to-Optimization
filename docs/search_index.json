[["index.html", "Introduction to Optimization Preface", " Introduction to Optimization Apurva Nakade 2022-01-10 Preface The question of optimization is the very general question of deciding when a function \\(g(x_1, \\dots, x_n)\\) attains its maximum or minimum value on a domain \\(D\\) in \\(\\mathbb{R^n}\\). \\[\\begin{align} \\mbox{optimize: } &amp;&amp; g(x_1, \\dots, x_n) &amp; \\\\ \\mbox{subject to: } &amp;&amp; (x_1, \\dots, x_n) &amp;\\in D. \\end{align}\\] These questions show up frequently in every quantitative field. But it is almost impossible to analyze the question at this level of generality without making any further assumptions on \\(g\\) and \\(D\\). We will start with the simplest interesting functions, namely, linear functions. We’ll let \\(g\\) be a linear function and let \\(D\\) be a region cut out by linear (in)equalities. The study of this problem is called Linear Programming. Despite the simplicity of the Linear Programming setup, or perhaps because of it, LP is one of the most commonly used models for real world optimization problems. Most of this course is about Linear Programming. In the first half, we’ll study the simplex method, which is an algorithm for solving linear programs, and in the second half we’ll study the theory behind linear programs using duality theory. "],["introduction-to-linear-programming.html", "Day 1 Introduction to Linear Programming 1.1 Resource allocation problem", " Day 1 Introduction to Linear Programming Linear programming is a technique for the optimization of a linear objective function, subject to linear (in)equality constraints. A linear program or a linear programming problem is a problem of the following form: \\[\\begin{equation} \\begin{aligned} \\mbox{optimize: } &amp;&amp; c_1 x_1 + \\dots + c_n x_n &amp; \\\\ \\mbox{subject to: } &amp;&amp; a_{11} x_1 + \\dots + a_{1n} x_n &amp; \\lesseqgtr b_1 \\\\ &amp;&amp; a_{21} x_1 + \\dots + a_{2n} x_n &amp; \\lesseqgtr b_2 \\\\ &amp;&amp; \\vdots &amp; \\\\ &amp;&amp; a_{m1} x_1 + \\dots + a_{mn} x_n &amp; \\lesseqgtr b_m, \\end{aligned} \\tag{1.1} \\end{equation}\\] where the symbol \\(\\lesseqgtr\\) stands for \\(\\leq\\) or \\(=\\) or \\(\\geq\\), \\(a_{ij}, b_i, c_j\\) are real numbers, and \\(x_j\\) are variables. The variables \\(x_1, \\dots, x_n\\) are called decision variables. The linear combination \\(\\zeta := c_1 x_1 + \\dots + c_n x_n\\) is called the objective function. Each of the (in)equalities \\(a_{i1} x_1 + \\dots + a_{in} x_n \\lesseqgtr b_i\\) is called a (linear) constraint. Our goal is to either minimize or maximize the objective function subject to the constraints. A proposal of specific values for the decision variables is called a solution. A solution is said to be feasible if it satisfies all the constraints. A feasible solution is said to be optimal if \\(\\zeta\\) attains the optimal value at it. Thus, to solve a linear program means to find an optimal solution to the problem. If a problem has no feasible solutions, then the problem is called infeasible. If a problem has feasible solutions with arbitrarily large or arbitrary small objective values then the problem is called unbounded. Remark. We do not allow strict inequalities \\(&lt;\\) or \\(&gt;\\) in a linear program as linear functions do not always achieve maxima/minima on open sets. Consider the following simple example. \\[\\begin{align} \\mbox{maximize: }&amp;&amp; 10 x_1 &amp; \\\\ \\mbox{subject to: } &amp;&amp; 10 x_1 &amp; &lt; 20 \\\\ &amp;&amp; 5 x_1 &amp; \\geq 0\\end{align}\\] On the feasible set \\([0, 2)\\), the function \\(\\zeta(x_1) = 10x_1\\) never attains absolute maxima. Changing the inequality \\(&lt;\\) to \\(\\leq\\) gives us an optimal feasible solution \\(x_1 = 2\\) and \\(\\zeta = 20\\). When all the (in)equalities are either \\(\\le\\), \\(=\\), or \\(\\ge\\), the set of feasible solutions is closed. If, in addition, the LP is bounded then the set of feasible solutions is compact. On compact sets a continuous (in particular, linear) function, always attains a maxima and a minima by a generalization of the extreme value theorem for higher dimensions. 1.1 Resource allocation problem The following is an example of a resource allocation problem, a very common application of linear programming. The decision variables \\(x_j\\) denote the amount of a certain resource/product \\(j\\), the coefficients \\(c_j\\) are the profit per unit quantity of \\(j\\), and the constraints are certain upper bounds on the production of the resource/quantity. Example 1.1 You run a company that makes two products (say, \\(P_1\\) and \\(P_2\\)) using two machines (say, \\(M_1\\) and \\(M_2\\)). Each unit of \\(P_1\\) that is produced requires 80 minutes processing time on machine \\(M_1\\) and 25 minutes processing time on machine \\(M_2\\). Each unit of \\(P_2\\) that is produced requires 20 minutes processing time on machine \\(M_1\\) and 75 minutes processing time on machine \\(M_2\\). Both machines are available for a maximum of 200 minutes every day. The profit per unit of \\(P_1\\) is 25 units and the profit per unit of \\(P_2\\) is 30 units. Company policy is to determine the production quantity of each product in such a way as to maximize the total profit given that the available resources should not be exceeded. P1 P2 Upper_Bounds M1 80 20 200 M2 25 75 200 We can formulate the above problem as the following linear program. \\[\\begin{align} \\begin{array}{rrrrrrrr} \\mbox{maximize: } &amp; 25x &amp; + &amp; 30y \\\\ \\mbox{subject to: } &amp; 80x &amp; + &amp; 20y &amp; \\le &amp; 200 \\\\ &amp; 25x &amp; + &amp; 75y &amp; \\le &amp; 200 \\\\ \\end{array} \\\\ x, \\: y \\ge 0\\end{align}\\] Here \\(x\\), \\(y\\) are the units of \\(P_1\\) and \\(P_2\\) produced each day. One way to attempt this problem is to assume that we’ll use both the machines as much as possible i.e. that the two inequalities are in fact equalities. We can see then that the common solution to the two is \\((x_1, x_2) = (2, 2)\\). For this solution the total profit is \\(110\\) units. Figure 1.1: The feasible region is the quadrilateral formed by the overlap of the constraints. Exercise 1.1 Why is this the maximum profit possible? Is it possible to increase the profit by not using either of the two machines at their full capacity? Exercise 1.2 Consider the setup in Example 1.1 again. Suppose in addition, you can only store a total of 3 units of \\(P_1\\) and \\(P_2\\) combined each day so your production quantity should not exceed that amount. What is the new LP and the new optimal solution? Note that the solution \\((x, y) = (2, 2)\\) is no longer possible. So, we cannot use both the machines to their maximum capacity. "],["geometry-equivalence-standard-form.html", "Day 2 Geometry, Equivalence, Standard form 2.1 Using geometry to solve linear programs 2.2 Equivalence of linear programs 2.3 Standard form of linear programs", " Day 2 Geometry, Equivalence, Standard form 2.1 Using geometry to solve linear programs Here is one way to see that \\((2, 2)\\) is the optimal solution to Example 1.1. For any constant \\(c\\), the equation \\[\\begin{equation} c = 25x + 30y \\tag{2.1} \\end{equation}\\] describes a line in \\(\\mathbb{R}^2\\). The points below this line have an objective value less than \\(c\\) and the points above this line have an objective value greater than \\(c\\). To see that \\((2, 2)\\) is the optimal solution, we simply need to see that the line of the form (2.1) that passes through it, namely \\(110 = 25x + 30y\\), lies above the feasible region as seen in the following figure. Figure 2.1: The feasible region lies below the line \\(110 = 25x + 30y\\) and intersects it at \\((2, 2)\\). Exercise 2.1 Find an objective function \\(\\zeta\\) for which the point \\((2, 2)\\) is no longer an optimal solution. We can use the same reasoning to solve Exercise 1.2, which is modelled by the following linear program. \\[\\begin{align} \\begin{array}{rrrrrrrr} \\mbox{maximize: } &amp; 25x &amp; + &amp; 30y \\\\ \\mbox{subject to: } &amp; 80x &amp; + &amp; 20y &amp; \\le &amp; 200 \\\\ &amp; 25x &amp; + &amp; 75y &amp; \\le &amp; 200 \\\\ &amp; 1x &amp; + &amp; 1y &amp; \\le &amp; 3 \\\\ \\end{array} \\\\ x, \\: y \\ge 0\\end{align}\\] The line of the form (2.1) for which the entire feasible region lies below it and which intersects the feasible region is \\(87.5 = 25x + 30y\\) and it passes through \\((0.5, 2.5)\\). Here, instead of maximizing the time spend on each machine, we’re maximizing the amount of resources produced and the time spent on the second machine \\(M_2\\). As a result, our profit dropped from \\(110\\) to \\(87.5\\). Figure 2.2: The feasible region lies below the line \\(87.5 = 25x + 30y\\) and intersects it at \\((0.5, 2.5)\\). In both the problems above, we found the line by inspection. There is no efficient way to do this purely algebraically! As a consequence, there is no way to create a useful algorithm out of this method. We’ll see a completely new way of approaching this problem using the simplex method. Instead of trying to find the optimal hyperplane, the simplex method jumps from vertex to vertex searching for the optimal solution. The existence of the optimal solution at a vertex is guaranteed by the following theorem. Theorem 2.1 Suppose the solution set of a linear program in two variables is non-empty and bounded. Then, The feasible set is a convex polygon. The optimal value is attained at a vertex of the feasible set. This theorem also holds in more than two variables, in which case instead of a convex polygon we get a convex polytope. The proof is in the following exercises. Exercise 2.2 Consider the following linear program \\[\\begin{align*} \\mbox{maximize: } &amp;&amp; c_1 x + c_2 y &amp; =: \\zeta(x,y) \\\\ \\mbox{subject to: } &amp;&amp; a_{11} x + a_{12} y &amp; \\lesseqgtr b_1 \\\\ &amp;&amp; a_{21} x + a_{22} y &amp; \\lesseqgtr b_2 \\\\ &amp;&amp; \\vdots &amp; \\\\ &amp;&amp; a_{m1} x + a_{m2} y &amp; \\lesseqgtr b_m. \\end{align*}\\] Let \\(\\mathcal{S}\\) denote the feasible set. Assume that \\(\\mathcal{S}\\) is bounded and non-empty. Let \\(P = (x_1, y_1)\\) and \\(Q = (x_2, y_2)\\) be two distinct points in \\(\\mathbb{R}^2\\). Let \\(R = (x_3, y_3)\\) be a point lying on the line segment between \\(P\\) and \\(Q\\). Show that \\(R = tP + (1-t)Q\\) for some \\(t\\) in \\((0, 1)\\). Show that if that both \\(P\\) and \\(Q\\) satisfy the constraints above, then \\(R\\) also satisfies the constraints. Conclude that if \\(P\\) and \\(Q\\) are in \\(\\mathcal{S}\\) then so is \\(R\\). A subset of \\(\\mathbb{R}^n\\) is called convex if, given any two points in the subset, the subset contains the whole line segment that joins them. The above exercises show that \\(\\mathcal{S}\\) is a convex subset of \\(\\mathbb{R}^2\\). Furthermore, because it is bounded and defined using linear equations, it is a . Show that \\(\\zeta(R) = t\\zeta(P) + (1-t)\\zeta(Q)\\) for some \\(t\\) in $ (0,1) $. Show that either \\(\\zeta(R) \\le \\zeta(P)\\) or \\(\\zeta(R) \\le \\zeta(Q)\\) (or both). Let \\(R&#39;\\) be a point in the interior of \\(\\mathcal{S}\\). Argue that there is a point \\(P&#39;\\) on the boundary of \\(\\mathcal{S}\\) such that \\(\\zeta(R&#39;) \\le \\zeta(P&#39;)\\). Let \\(R&#39;&#39;\\) be a point in the interior of one of the edges of \\(\\mathcal{S}\\). Argue that there is a vertex \\(P&#39;&#39;\\) of \\(\\mathcal{S}\\) such that \\(\\zeta(R&#39;&#39;) \\le \\zeta(P&#39;&#39;)\\). Conclude that \\(\\zeta\\) attains its maximum value at a vertex of \\(\\mathcal{S}\\). 2.2 Equivalence of linear programs Before we get to the simplex method, we’ll need to standardize our linear programs. The first step is to notice that every linear program can be changed to a maximization problem as minimizing a function \\(\\zeta\\) is the same as maximizing the function \\(-\\zeta\\). From now on, we’ll assume that the goal of our linear programs is to maximize the objective function. Two (maximizing) linear programs LP and LP’ are said to be equivalent if for any feasible solution \\((x_1, \\dots, x_n)\\) to LP, there exists a feasible solution \\((x&#39;_1, x&#39;_2, \\dots, x&#39;_{n&#39;})\\) to LP’ with the same objective value \\[ \\zeta(x_1, \\dots, x_n) = \\zeta&#39;(x&#39;_1, x&#39;_2, \\dots, x&#39;_{n&#39;}), \\] and vice versa. Thus solving LP is equivalent to LP’. Remark.   LP and LP’ can have a different number of decision variables i.e. we do not require \\(n = n&#39;.\\) There need not be a one-to-one correspondence between the feasible sets of LP and LP’ i.e. for a feasible solution to LP there could be multiple feasible solutions with the same objective value. Similarly, in the other direction. Remark. Even though equivalence of linear programs only requires the existence of an abstract correspondence between the feasible sets of LP and LP’, in practice, one constructs linear transformations \\(T: \\mathbb{R}^n \\to \\mathbb{R}^{n&#39;}\\) and \\(S: \\mathbb{R}^{n&#39;} \\to \\mathbb{R}^{n}\\) which map the feasible set of LP to LP’ and the feasible set of LP’ to LP, respectively. These linear transformations need not be inverses of each other, or even isomorphisms. They only need to preserve the objective values. Example 2.1 The following linear programs are all equivalent to the linear program in Example 1.1. \\[\\begin{align} \\begin{array}{rrrrrrrr} \\mbox{maximize: } &amp; 250x &amp; + &amp; 300y \\\\ \\mbox{subject to: } &amp; 80x &amp; + &amp; 20y &amp; \\le &amp; 20 \\\\ &amp; 25x &amp; + &amp; 75y &amp; \\le &amp; 20 \\\\ \\end{array} \\\\ x, \\: y \\ge 0\\end{align}\\] \\[\\begin{align} \\begin{array}{rrrrrrrr} \\mbox{maximize: } &amp; 25x &amp; + &amp; 30y \\\\ \\mbox{subject to: } &amp; 800x &amp; + &amp; 200y &amp; \\le &amp; 2000 \\\\ &amp; 25x &amp; + &amp; 75y &amp; \\le &amp; 200 \\\\ \\end{array} \\\\ x, \\: y \\ge 0\\end{align}\\] \\[\\begin{align} \\begin{array}{rrrrrrrr} \\mbox{maximize: } &amp; 25x &amp; + &amp; 30y \\\\ \\mbox{subject to: } &amp; 80x &amp; + &amp; 20y &amp; \\le &amp; 200 \\\\ &amp; 25x &amp; + &amp; 75y &amp; \\le &amp; 200 \\\\ &amp; 2x &amp; + &amp; 2y &amp; \\le &amp; 200 \\\\ \\end{array} \\\\ x, \\: y \\ge 0\\end{align}\\] Example 2.2 The following two linear programs are equivalent to each other \\[\\begin{align*} \\begin{aligned} \\mbox{maximize: } &amp;&amp; x + y &amp; \\\\ \\mbox{subject to: } &amp;&amp; 0 \\le x &amp;\\le 1 \\\\ &amp;&amp; 0 \\le y &amp;\\le 1 \\end{aligned} &amp;&amp; \\begin{aligned} \\mbox{maximize: } &amp;&amp; z &amp; \\\\ \\mbox{subject to: } &amp;&amp; 0 \\le z &amp;\\le 1 \\end{aligned} \\end{align*}\\] via the transformations \\(T(x, y) = x + y\\) and \\(S(z) = (z/2, z/2)\\). 2.3 Standard form of linear programs A linear program of the following form is said to be in a standard form: \\[\\begin{equation} \\begin{array}{lrrrrrrrrr} \\mbox{maximize: } &amp; c_1 x_1 &amp; + &amp; \\dots &amp; + &amp; c_n x_n &amp; \\\\ \\mbox{subject to: } &amp; a_{11} x_1 &amp; + &amp; \\dots &amp; + &amp; a_{1n} x_n &amp; \\leq &amp; b_1 \\\\ &amp; a_{21} x_1 &amp; + &amp; \\dots &amp; + &amp; a_{2n} x_n &amp; \\leq &amp; b_2 \\\\ &amp; \\vdots &amp; \\\\ &amp; a_{m1} x_1 &amp; + &amp; \\dots &amp; + &amp; a_{mn} x_n &amp; \\leq &amp; b_m \\end{array} \\\\ x_1, \\: \\dots \\: , \\: x_{n} \\: \\geq \\: 0 \\tag{2.2} \\end{equation}\\] Such a linear program can be written more succinctly using vectors and matrices as follows. \\[\\begin{equation} \\begin{aligned} \\mbox{maximize: } &amp;&amp; c^T x &amp; \\\\ \\mbox{subject to: } &amp;&amp; A x &amp; \\leq b \\\\ &amp;&amp; x &amp; \\geq 0. \\end{aligned} \\tag{2.3} \\end{equation}\\] where \\(x\\) is now the vector of decision variables, \\(c\\) and \\(b\\) are vectors of real numbers, and \\(A\\) is the matrix of constraint coefficients. This enables us to use tools from Linear Algebra to solve linear programs. Theorem 2.2 Every linear program is equivalent to a linear program in the standard form. Proof. The proof is by an explicit algorithm. Consider the linear program in (1.1), where we’re assuming that the goal is to maximize the objective function. If it is in the standard form, then we’re done. If not, then there must be a finite number of errors of the following types: A linear constraint is an upper bound and has the form \\[a_{i1} x_1 + \\dots + a_{in} x_n \\geq b_i.\\] A linear constraint is an equality and has the form \\[a_{i1} x_1 + \\dots + a_{in} x_n = b_i.\\] A variable \\(x_j\\) has a negativity constraint \\(x_j \\leq 0\\). A variable \\(x_j\\) is missing a sign constraint. We fix each error sequentially while making sure that no new errors are introduced, thereby ensuring termination of the algorithm. We replace the constraint with \\[ -a_{i1} x_1 + \\dots + -a_{in} x_n \\leq -b_i. \\] We replace the constraint with two constraints \\[\\begin{align*} a_{i1} x_1 + \\dots + a_{in} x_n &amp;\\leq b_i \\\\ -a_{i1} x_1 - \\dots - a_{in} x_n &amp;\\leq -b_i, \\end{align*}\\] We let \\(y_j = -x_j\\) and create a new linear program using the variables \\(x_1\\), \\(\\dots\\), \\(x_{j-1}\\), \\(y_j\\), \\(x_{j+1}\\), \\(\\dots\\), \\(x_n\\) by replacing \\(x_j\\) with \\(-y_j\\) everywhere. We let \\(x_j = y_j - z_j\\) for two new decision variables \\(y_j\\) and \\(z_j\\) with \\(y_j, z_j \\geq 0\\) and create a new linear program using the variables \\(x_1\\), \\(\\dots\\), \\(x_{j-1}\\), \\(y_j\\), \\(z_j\\), \\(x_{j+1}\\), \\(\\dots\\), \\(x_n\\) by replacing \\(x_j\\) with \\(y_j - z_j\\) everywhere. We can do this because any real number can written as a difference of two positive real numbers. One can show that in each step the modified LP is equivalent to the original LP. Exercise 2.3 Prove that the algorithm in the proof of Theorem 2.2 produces a linear program that is equivalent to the original linear program. "],["simplex-method---example.html", "Day 3 Simplex Method - Example", " Day 3 Simplex Method - Example The simplex method jumps from vertex of the feasible region to find the optimal solution. It is best demonstrated using an example. Consider the following linear program. \\[\\begin{equation} \\begin{array}{rrrrrrrr} \\mbox{maximize: } &amp; 1x_1 &amp; + &amp; 1x_2 \\\\ \\mbox{subject to: } &amp; 1x_1 &amp; + &amp; 2x_2 &amp; \\le &amp; 10 \\\\ &amp; 4x_1 &amp; + &amp; 3x_2 &amp; \\le &amp; 20 \\\\ \\end{array} \\\\ x_1, \\: x_2 \\ge 0 \\tag{3.1} \\end{equation}\\] The optimal solution is \\((x_1, x_2) = (2, 4)\\) with objective value \\(6\\). There are six different points at which the lines corresponding to the constraints intersect. We’ll start with the vertex \\((0,0)\\) and make our way up to \\((2, 4)\\) while making sure that we do not leave the feasible region. The first, and arguably the most important trick in the simplex algorithm is the introduction of slack variables \\(x_3\\) and \\(x_4\\), one for each constraint, as shown below, to create a new linear program called a dictionary which is equivalent to (3.1) \\[\\begin{equation} \\begin{array}{rrrrrrrr} \\mbox{maximize: } &amp; \\zeta &amp;=&amp;0&amp; + &amp;1x_1 &amp;+&amp; 1x_2 \\\\ \\mbox{subject to: } &amp; x_3 &amp; = &amp; 10 &amp; - &amp;1x_1 &amp; - &amp;2x_2 \\\\ &amp; x_4 &amp; = &amp; 20 &amp; - &amp;4x_1 &amp; - &amp;3x_2 \\\\ \\end{array} \\\\ x_1, \\: x_2, \\: x_3, \\: x_4 \\ge 0 \\tag{3.2} \\end{equation}\\] Using these new variables, the points of intersection can be written as intersections of the lines \\(x_i = 0\\) and \\(x_j = 0\\), as \\(i\\) and \\(j\\) vary over the index set \\(\\{1, 2, 3, 4\\}\\). And so to traverse the set of vertices, we simply need to traverse the two element subsets of \\(\\{x_1 = 0, x_2 = 0, x_3 = 0, x_4 = 0\\}\\). We start with the guess: \\(x_1 = 0, x_2 = 0, x_3 = 10, x_4 = 20\\). At this point, \\(\\zeta = 0\\). We can increase \\(\\zeta\\) by increasing either \\(x_1\\) or \\(x_2\\). Let’s choose \\(x_1\\) and leave \\(x_2\\) fixed at \\(0\\). To not violate the positivity of \\(x_3\\), \\(x_4\\), we can only increase \\(x_1\\) enough that both \\(10 \\ge x_1\\) and \\(20 \\ge 4x_1\\) remain true. The maximum such \\(x_1\\) is \\(5\\) and this makes \\(x_4 = 0\\). We rewrite \\(x_1\\), \\(x_3\\), and \\(\\zeta\\) in terms of the new variables that are \\(0\\), namely \\(x_2\\) and \\(x_4\\), to create a new dictionary: \\[\\begin{equation} \\begin{array}{rrrrrrrr} \\mbox{maximize: } &amp; \\zeta &amp;=&amp;5&amp; + &amp;-0.25x_4 &amp;+&amp; 0.25x_2 \\\\ \\mbox{subject to: } &amp; x_3 &amp; = &amp; 5 &amp; - &amp;-0.25x_4 &amp; - &amp;1.25x_2 \\\\ &amp; x_1 &amp; = &amp; 5 &amp; - &amp;0.25x_4 &amp; - &amp;0.75x_2 \\\\ \\end{array} \\\\ x_4, \\: x_2, \\: x_3, \\: x_1 \\ge 0 \\tag{3.3} \\end{equation}\\] Our new guess is: \\(x_1 = 5, x_2 = 0, x_3 = 5, x_4 = 0\\). The coefficient of \\(x_4\\) in the objective function is negative. Increasing it will only decrease the objective value. The only variable we can increase in the objective function is \\(x_2\\). To not violate the positivity of \\(x_1\\), \\(x_3\\), we can only increase \\(x_2\\) enough that both \\(5 \\ge 1.25x_2\\) and \\(5 \\ge 0.75x_2\\) remain true. The maximum such \\(x_2\\) is \\(\\min(5/1.25, 5/0.75) = 5/1.25 = 4\\) and this makes \\(x_3 = 0\\). We rewrite \\(x_2\\), \\(x_1\\), and \\(\\zeta\\) in terms of the new variables that are \\(0\\), namely \\(x_3\\) and \\(x_4\\), to create a new dictionary: \\[\\begin{equation} \\begin{array}{rrrrrrrr} \\mbox{maximize: } &amp; \\zeta &amp;=&amp;6&amp; + &amp;-0.2x_4 &amp;+&amp; -0.2x_3 \\\\ \\mbox{subject to: } &amp; x_2 &amp; = &amp; 4 &amp; - &amp;-0.2x_4 &amp; - &amp;0.8x_3 \\\\ &amp; x_1 &amp; = &amp; 2 &amp; - &amp;0.4x_4 &amp; - &amp;-0.6x_3 \\\\ \\end{array} \\\\ x_4, \\: x_3, \\: x_2, \\: x_1 \\ge 0 \\tag{3.4} \\end{equation}\\] Our new guess is: \\(x_1 = 2, x_2 = 4, x_3 = 0, x_4 = 0\\). The coefficients of both the variables that appear in \\(\\zeta\\) are negative. It is not possible to increase either variable without decreasing the objective value. The simplex algorithm halts. The optimal objective value \\(6\\) is the constant coefficient in the final \\(\\zeta\\) and it is attained at \\((x_1, x_2) = (2, 4)\\). "],["two-phase-simplex-method.html", "Day 4 Two Phase Simplex Method 4.1 Phase II 4.2 Phase I", " Day 4 Two Phase Simplex Method As we saw in Example (3.1), the two phase simplex method is an algorithm for solving a linear program in the standard form. The method described below is the Two phase simplex method using the dictionary notation. We’ll later see an implementation of the same method using the tableaux notation. Given a standard linear program (2.2), we can form an linear program called dictionary of the following form by introducing slack variables \\(x_{n+1}, \\dots, x_{n + m}\\): \\[\\begin{equation} \\begin{array}{rrrrrrrrrrrr} \\mbox{maximize: } &amp; \\zeta &amp; = &amp; c _ 0 &amp; + &amp; c_1 x_1 &amp; + &amp; \\dots &amp; + &amp; c_n x_n \\\\ \\mbox{subject to: } &amp; x_{n+1} &amp; = &amp; b_1 &amp; - &amp; a_{11} x_1 &amp; - &amp; \\dots &amp; - &amp; a_{1n} x_n \\\\ &amp; x_{n+2} &amp; = &amp; b_2 &amp; - &amp; a_{21} x_1 &amp; - &amp; \\dots &amp; - &amp; a_{2n} x_n \\\\ &amp; &amp; &amp; &amp; &amp; &amp; \\vdots &amp; &amp; \\\\ &amp; x_{n+m} &amp; = &amp; b_m &amp; - &amp; a_{m1} x_1 &amp; - &amp; \\dots &amp; - &amp; a_{mn} x_n \\end{array} \\\\ x_1, \\: \\dots \\: , \\: x_{m + n} \\: \\geq \\: 0 \\tag{4.1} \\end{equation}\\] The variables appearing on the right-hand side are called non-basic variables, and the variables appearing on the left-hand side are called basic variables. The simplex algorithm creates a sequence of dictionaries through a process called pivoting. In each pivot step, one non-basic variable becomes basic; this is the entering variable, and one basic variable becomes non-basic; this is the leaving variable. A basic solution of the dictionary is obtained by setting all the non-basic variables to 0. If such a solution is feasible, then it is called a basic feasible solution (BFS for short). In this case, we say that the dictionary is feasible. Exercise 4.1 Show that a dictionary is feasible if and only if all the constants \\(b_i\\) are non-negative. 4.1 Phase II Phase II of the simplex method assumes that the dictionary is feasible. If this is not the case, we use Phase I to try and find a feasible dictionary. Once we have a feasible dictionary, we proceed as follows. Find entering variable: Let \\(e\\) be an index such that \\(c_e\\) is positive. If none exists, exit. Find leaving variable: Let \\(S\\) be the set of indices \\(i\\) such that \\(a_{ie}\\) is positive. Set \\(\\ell = \\mathrm{arg}\\max_{i \\in S} \\left(\\dfrac{b_i}{a_{ie}}\\right)\\). Pivot: Add \\(x_e\\) to the set of basic variables and remove it from the set of non-basic variables. Add \\(x_\\ell\\) to the set of non-basic variables and remove it from the set of basic variables. Rewrite the basic variables and the objective function in terms of the non-basic variables. Go back to step 1. Assuming the program halts, the optimal solution will be the BFS of the final dictionary. 4.2 Phase I In the case when some of the constants \\(b_i\\) are negative, Phase I of the simplex algorithm is used to find a feasible dictionary. It attempts to find a sequence of pivots on the dictionary (4.1) that make it feasible. The trick is to create the following auxiliary linear program whose optimal solution provides a BFS of the original linear program, if one exists. \\[\\begin{equation} \\begin{array}{rrrrrrrrrrrr} \\mbox{maximize: } &amp; &amp; &amp; &amp; &amp; &amp; - &amp; x_0 \\\\ \\mbox{subject to: } &amp; a_{11} x_1 &amp; + &amp; \\dots &amp; + &amp; a_{1n} x_n &amp; - &amp; x_0 &amp; \\le &amp; b_1 \\\\ &amp; a_{21} x_1 &amp; + &amp; \\dots &amp; + &amp; a_{2n} x_n &amp; - &amp;x_0 &amp; \\le &amp; b_2 \\\\ &amp; &amp; &amp; &amp; &amp; &amp; \\vdots &amp; &amp; \\\\ &amp; a_{m1} x_1 &amp; + &amp; \\dots &amp; + &amp; a_{mn} x_n &amp; - &amp;x_0 &amp; \\le &amp; b_m \\end{array} \\\\ x_0, \\: x_1, \\: \\dots \\: , \\: x_{n} \\: \\geq \\: 0 \\tag{4.2} \\end{equation}\\] We create an (infeasible) dictionary for this linear program and process as follows. Find leaving variable: Set \\(\\ell = \\mathrm{arg}\\min_{1 \\le i \\le m} b_i\\). Pivot: Add \\(x_0\\) to the set of basic variables and remove it from the set of non-basic variables. Add \\(x_\\ell\\) to the set of non-basic variables and remove it from the set of basic variables. Rewrite the basic variables and the objective function in terms of the non-basic variables. Solve: Apply the Phase II algorithm to find an optimal solution. In the optimal solution, If \\(x_0 \\neq 0\\), then the original linear program is not feasible. If \\(x_0 = 0\\), Form the initial infeasible dictionary from the original linear program. Perform pivots until the optimal solution of the auxiliary linear program becomes the BFS of the resulting dictionary. Proceed to Phase II. Step 2 of the above algorithm creates a feasible dictionary by Exercise 4.3. Hence, we can use the Phase II algorithm to solve the auxiliary linear program in Step 3. The solution obtained in Step 3 provides a BFS of the original linear program when \\(x_0 = 0\\) by the proof of Part 4 of Exercise 4.2. Exercise 4.2 The following exercises prove that optimal solution of the auxiliary linear program provides a BFS of the original linear program. Show that the linear program (4.2) is always feasible. Assume that the linear program (4.2) always has an optimal solution. Show that the optimal objective value of (4.2) is always \\(\\le 0\\). Show that if the linear program (2.2) is feasible then the optimal objective value of (4.2) is 0. Show that if the optimal objective value of (4.2) is 0 then (2.2) is feasible. Exercise 4.3 Suppose the dictionary of (2.2) (and hence (4.2)) is not feasible. Without any loss of generality, assume that \\(b_1\\) is the smallest among all the \\(b_i\\) (i.e. the most negative). Show that the initial dictionary of (4.2) becomes feasible after one pivot step that makes \\(x_0\\) basic and \\(x_{n+1}\\) non-basic. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
