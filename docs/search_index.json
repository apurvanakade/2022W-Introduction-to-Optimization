[["index.html", "Introduction to Optimization Preface", " Introduction to Optimization Apurva Nakade 2021-12-28 Preface The question of optimization is the very general question of deciding when a function \\(g(x_1, \\dots, x_n)\\) attain its maximum or minimum value on a domain \\(D\\) in \\(\\mathbb{R^n}\\). \\[\\begin{align} \\mbox{optimize: } &amp;&amp; g(x_1, \\dots, x_n) &amp; \\\\ \\mbox{subject to: } &amp;&amp; (x_1, \\dots, x_n) &amp;\\in D. \\end{align}\\] These questions show up frequently in almost every quantitative field. But it is nigh impossible to analyze the question at this level of generality without making any further assumptions on \\(g\\) and \\(D\\). We will start with the simplest interesting functions i.e. linear functions. We’ll let \\(g\\) be a linear function and let \\(D\\) be a region cut out by linear inequalities. The study of this problem is called Linear Programming. In spite of the simplicity of the Linear Programming setup, or perhaps because of it, LP is one of the most common used models for real world optimization questions. "],["introduction-to-linear-programming.html", "Day 1 Introduction to Linear Programming 1.1 Resource allocation problem", " Day 1 Introduction to Linear Programming Linear programming is a technique for the optimization of a linear objective function, subject to linear equality and linear inequality constraints. A linear program or a linear programming problem is a problem of the following form: \\[\\begin{equation} \\begin{aligned} \\mbox{optimize: } &amp;&amp; c_1 x_1 + \\dots + c_n x_n &amp; \\\\ \\mbox{subject to: } &amp;&amp; a_{11} x_1 + \\dots + a_{1n} x_n &amp; \\lesseqgtr b_1 \\\\ &amp;&amp; a_{21} x_1 + \\dots + a_{2n} x_n &amp; \\lesseqgtr b_2 \\\\ &amp;&amp; \\vdots &amp; \\\\ &amp;&amp; a_{m1} x_1 + \\dots + a_{mn} x_n &amp; \\lesseqgtr b_m, \\end{aligned} \\tag{1.1} \\end{equation}\\] where the symbol \\(\\lesseqgtr\\) stands for \\(\\leq\\) or \\(=\\) or \\(\\geq\\), \\(a_{ij}, b_i, c_j\\) are real numbers, and \\(x_j\\) are variables. The variables \\(x_1, \\dots, x_n\\) are called decision variables. The linear combination \\(\\zeta := c_1 x_1 + \\dots + c_n x_n\\) is called the objective function. Each of the equalities/inequalities \\(a_{i1} x_1 + \\dots + a_{in} x_n \\lesseqgtr b_i\\) is called a (linear) constraint. Our goal is to either minimize or maximize the objective function subject to the constraints. A proposal of specific values for the decision variables is called a solution. A solution is said to be feasible if it satisfies all the constraints. A feasible solution is said to be optimal if \\(\\zeta\\) attains the optimal value at the solution. Thus, to solve a linear program means to find an optimal solution to the problem. If a problem has no feasible solutions, then the problem is called infeasible. If a problem has feasible solutions with arbitrarily large or arbitrary small objective values then the problem is called unbounded. Remark. We do not allow strict inequalities \\(&lt;\\) or \\(&gt;\\) in a linear program as linear functions do not always achieve maxima/minima on open sets. Consider the following simple example. \\[\\begin{align*} \\mbox{maximize: }&amp;&amp; 10 x_1 &amp; \\\\ \\mbox{subject to: } &amp;&amp; 10 x_1 &amp; &lt; 20 \\\\ &amp;&amp; 5 x_1 &amp; \\geq 0 \\end{align*}\\] On the feasible set \\([0, 2)\\), the function \\(\\zeta(x_1) = x_1\\) never attains absolute maxima. Changing the inequality \\(&lt;\\) to \\(\\leq\\) gives us an optimal feasible solution \\(x_1 = 2\\). When all the inequalities are either \\(\\le\\), \\(=\\), or \\(\\ge\\), the set of feasible solutions is closed. If, in addition, the LP is bounded then the set of feasible solutions is compact. On compact sets a continuous (in particular, linear) function, always attains a maxima and a minima by a generalization of the extreme value theorem for higher dimensions. 1.1 Resource allocation problem It is useful to think of the objective function as the profit function, the decision variable \\(x_j\\) as the amount of a physical quantity of type \\(j\\), the coefficients \\(c_j\\) as being the profit made from selling a unit quantity of type \\(j\\), and each constraint as describing an upper bound on the production of an intermediate quantity. This is called a resource allocation problem. Consider the following situation. Suppose you want to buy shares in two mutual funds, let’s call them \\(A\\) and \\(B\\), currently priced at $100 and $75, respectively. Suppose you have a total of $500 to invest. You predict that in a year you’ll make \\(15\\%\\) profit on \\(A\\) and \\(10\\%\\) profit on \\(B\\). But there is a commission fee of Assuming you want to make the most profit, your optimization function becomes: "],["standard-form-of-a-linear-program.html", "Day 2 Standard form of a linear program Exercises", " Day 2 Standard form of a linear program It is inconvenient to work with a linear program in the form described in Equation (1.1) because each of the three kinds of constraints \\(\\leq\\), \\(=\\), and \\(\\geq\\) require different manipulations. A linear program of the following form is said to be in a standard form: \\[\\begin{equation} \\begin{aligned} \\mbox{maximize: } &amp;&amp; c_1 x_1 + \\dots + c_n x_n &amp; \\\\ \\mbox{subject to: } &amp;&amp; a_{11} x_1 + \\dots + a_{1n} x_n &amp; \\leq b_1 \\\\ &amp;&amp; a_{21} x_1 + \\dots + a_{2n} x_n &amp; \\leq b_2 \\\\ &amp;&amp; \\vdots &amp; \\\\ &amp;&amp; a_{m1} x_1 + \\dots + a_{mn} x_n &amp; \\leq b_m \\\\ &amp;&amp; x_1, \\dots, x_n &amp; \\geq 0. \\end{aligned} \\tag{2.1} \\end{equation}\\] The last line is a succinct way of writing the \\(n\\) different inequalities \\(x_j \\geq 0\\) for \\(j = 1, \\dots, n\\). A linear program in a standard form can be written more succinctly using vectors and matrices as follows. \\[\\begin{equation} \\begin{aligned} \\mbox{maximize: } &amp;&amp; c^T x &amp; \\\\ \\mbox{subject to: } &amp;&amp; A x &amp; \\leq b \\\\ &amp;&amp; x &amp; \\geq 0. \\end{aligned} \\tag{2.2} \\end{equation}\\] where now \\(x\\) is the vector of decision variables, \\(c\\) and \\(b\\) are vectors of real numbers, and \\(A\\) is the matrix of constraint coefficients. This enables us to use tools from Linear Algebra to solve LP problems. 2.0.1 Converting LP to standard form Notice that maximizing the objective function \\(\\zeta\\) is equivalent to minimizing the negative of the objective function \\(-\\zeta\\). From now on, we will assume that we are always trying to maximize the objective function. Two linear programs LP and LP’ are said to be equivalent if for any feasible solution \\((x_1, \\dots, x_n)\\) to LP, there exists a feasible solution \\((x&#39;_1, x&#39;_2, \\dots, x&#39;_{n&#39;})\\) to LP’ such that \\(\\zeta(x_1, \\dots, x_n) = \\zeta&#39;(x&#39;_1, x&#39;_2, \\dots, x&#39;_{n&#39;})\\) and vice versa. Thus solving LP is equivalent to LP’. LP and LP’ can have a different number of decision variables i.e. it is not necessarily true that \\(n = n&#39;.\\) We do not require a one-to-one correspondence between the solutions of LP and LP’ i.e. for a feasible solution \\((x_1, \\dots, x_n)\\) to LP there could potentially be multiple feasible solutions \\((x_1&#39;, \\dots, x_n&#39;)\\) to LP’ satisfying \\(\\zeta(x_1, \\dots, x_n) = \\zeta&#39;(x&#39;_1, x&#39;_2, \\dots, x&#39;_{n&#39;})\\). Similarly, in the other direction. Theorem 2.1 Every linear program is equivalent to a linear program in the standard form. Proof. The proof is by explicit construction. Consider the LP in (1.1). If LP is in the standard form, then we’re done. If not, then one of the following conditions about the constraints must be true: One of the linear constraints is an upper bound and has the form \\[a_{i1} x_1 + \\dots + a_{in} x_n \\geq b_i.\\] One of the linear constraints is an equality and has the form \\[a_{i1} x_1 + \\dots + a_{in} x_n = b_i.\\] One of the variables has a “negativity constraint” \\(x_j \\leq 0\\). A “positivity constraint” \\(x_j \\geq 0\\) is missing. Note that we’re already assuming that the objective function is “fixed” i.e. that our goal is to maximize \\(\\zeta\\). We “fix” each of the above errors sequentially: Case 1: We multiply both sides by \\(-1\\) giving us the desired inequality \\[-a_{i1} x_1 + \\dots + -a_{in} x_n \\leq -b_i.\\] Case 2: We replace \\(a_{i1} x_1 + \\dots + a_{in} x_n = b_i\\) with two inequalities \\[\\begin{align*} a_{i1} x_1 + \\dots + a_{in} x_n &amp;\\leq b_i \\\\ a_{i1} x_1 + \\dots + a_{in} x_n &amp;\\geq b_i \\end{align*}\\] which reduces the problem to Case 1. Case 3: We simply replace \\(x_j\\) with \\(-x_j\\) everywhere. Case 4: We let \\(x_j = x_j&#39; - x_j&#39;&#39;\\) for two new decision variables \\(x_j&#39;\\) and \\(x_j&#39;&#39;\\) where \\(x_j&#39;, x_j&#39;&#39; \\geq 0\\). We can do this because any real number can written as a difference of two positive real numbers. One can show that in each step the modified LP is equivalent to the original LP (Exercise 2.2). Exercises Exercise 2.1 Prove that the equivalence of linear programs is an equivalence relation. Exercise 2.2 Prove that the algorithm in the proof of Theorem 2.1 produces a linear program that is equivalent to the original linear program. "],["simplex-method.html", "Day 3 Simplex Method", " Day 3 Simplex Method In a first course in Linear Algebra, you learn to solve equations of the form \\(A x = b\\). In LP, we are increasing the complexity on two levels. We are replacing \\(Ax = b\\) with \\(Ax \\leq b\\) and \\(x \\ge 0\\). Solving these equations will give us the set of all feasible solutions. Among all the feasible solutions we then need to find an optimal feasible solution. As it turns out, unlike in the Linear Algebra case, where the solutions to \\(Ax = b\\) forms an affine space of a finite dimension and can be described by a basis, it is not easy to describe the solutions to \\(Ax \\leq b\\) and \\(x \\ge 0\\) succinctly. So, we abandon the quest for finding all feasible solutions, for now. Instead, we will directly try to find optimal feasible solutions. The strategy for solving LP, which is mostly brute force, is called the simplex method. Implementation details of the simplex method vary a lot, but the general idea is the following. Guess a feasible solution. Check if there is a direction along which the objective value increases while staying feasible. If yes, then perturb the guess as much as possible in that direction and repeat. If no, then we have found a local maxima. Because all our functions are linear, a local maxima will also be a global maxima and the solution will be a feasible solution. Exercise 3.1 What would be some of the difficulties in running the above algorithm? "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
