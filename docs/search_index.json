[["index.html", "Introduction to Optimization Preface", " Introduction to Optimization Apurva Nakade 2022-01-03 Preface The question of optimization is the very general question of deciding when a function \\(g(x_1, \\dots, x_n)\\) attain its maximum or minimum value on a domain \\(D\\) in \\(\\mathbb{R^n}\\). \\[\\begin{align} \\mbox{optimize: } &amp;&amp; g(x_1, \\dots, x_n) &amp; \\\\ \\mbox{subject to: } &amp;&amp; (x_1, \\dots, x_n) &amp;\\in D. \\end{align}\\] These questions show up frequently in almost every quantitative field. But it is almost impossible to analyze the question at this level of generality without making any further assumptions on \\(g\\) and \\(D\\). We will start with the simplest interesting functions, namely, linear functions. We’ll let \\(g\\) be a linear function and let \\(D\\) be a region cut out by linear (in)equalities. The study of this problem is called Linear Programming. Despite the simplicity of the Linear Programming setup, or perhaps because of it, LP is one of the most commonly used models for real world optimization problems. Most of this course is about Linear Programming. In the first half, we’ll study the simplex method, which is an algorithm for solving linear programs, and the in the second half we’ll study the theory behind linear programs using duality theory. "],["introduction-to-linear-programming.html", "Day 1 Introduction to Linear Programming 1.1 Resource allocation problem", " Day 1 Introduction to Linear Programming Linear programming is a technique for the optimization of a linear objective function, subject to linear equality and linear inequality constraints. A linear program or a linear programming problem is a problem of the following form: \\[\\begin{equation} \\begin{aligned} \\mbox{optimize: } &amp;&amp; c_1 x_1 + \\dots + c_n x_n &amp; \\\\ \\mbox{subject to: } &amp;&amp; a_{11} x_1 + \\dots + a_{1n} x_n &amp; \\lesseqgtr b_1 \\\\ &amp;&amp; a_{21} x_1 + \\dots + a_{2n} x_n &amp; \\lesseqgtr b_2 \\\\ &amp;&amp; \\vdots &amp; \\\\ &amp;&amp; a_{m1} x_1 + \\dots + a_{mn} x_n &amp; \\lesseqgtr b_m, \\end{aligned} \\tag{1.1} \\end{equation}\\] where the symbol \\(\\lesseqgtr\\) stands for \\(\\leq\\) or \\(=\\) or \\(\\geq\\), \\(a_{ij}, b_i, c_j\\) are real numbers, and \\(x_j\\) are variables. The variables \\(x_1, \\dots, x_n\\) are called decision variables. The linear combination \\(\\zeta := c_1 x_1 + \\dots + c_n x_n\\) is called the objective function. Each of the equalities/inequalities \\(a_{i1} x_1 + \\dots + a_{in} x_n \\lesseqgtr b_i\\) is called a (linear) constraint. Our goal is to either minimize or maximize the objective function subject to the constraints. A proposal of specific values for the decision variables is called a solution. A solution is said to be feasible if it satisfies all the constraints. A feasible solution is said to be optimal if \\(\\zeta\\) attains the optimal value at it. Thus, to solve a linear program means to find an optimal solution to the problem. If a problem has no feasible solutions, then the problem is called infeasible. If a problem has feasible solutions with arbitrarily large or arbitrary small objective values then the problem is called unbounded. Remark. We do not allow strict inequalities \\(&lt;\\) or \\(&gt;\\) in a linear program as linear functions do not always achieve maxima/minima on open sets. Consider the following simple example. \\[\\begin{align*} \\mbox{maximize: }&amp;&amp; 10 x_1 &amp; \\\\ \\mbox{subject to: } &amp;&amp; 10 x_1 &amp; &lt; 20 \\\\ &amp;&amp; 5 x_1 &amp; \\geq 0 \\end{align*}\\] On the feasible set \\([0, 2)\\), the function \\(\\zeta(x_1) = x_1\\) never attains absolute maxima. Changing the inequality \\(&lt;\\) to \\(\\leq\\) gives us an optimal feasible solution \\(x_1 = 2\\). When all the inequalities are either \\(\\le\\), \\(=\\), or \\(\\ge\\), the set of feasible solutions is closed. If, in addition, the LP is bounded then the set of feasible solutions is compact. On compact sets a continuous (in particular, linear) function, always attains a maxima and a minima by a generalization of the extreme value theorem for higher dimensions. 1.1 Resource allocation problem The following is an example of a resource allocation problem, a very common application of linear programming. The decision variables \\(x_j\\) denote the amount of a certain resource/product \\(j\\), the coefficients \\(c_j\\) are the profit per unit quantity of \\(j\\), and the constraints are certain upper bounds on the production of the resource/quantity. Example 1.1 You run a company that makes two products (say, \\(P_1\\) and \\(P_2\\)) using two machines (say, \\(M_1\\) and \\(M_2\\)). Each unit of \\(P_1\\) that is produced requires 80 minutes processing time on machine \\(M_1\\) and 25 minutes processing time on machine \\(M_2\\). Each unit of \\(P_2\\) that is produced requires 20 minutes processing time on machine \\(M_1\\) and 75 minutes processing time on machine \\(M_2\\). Both machines are available for a maximum of 200 minutes every day. The profit per unit of \\(P_1\\) is 25 units and the profit per unit of \\(P_2\\) is 30 units. Company policy is to determine the production quantity of each product in such a way as to maximize the total profit given that the available resources should not be exceeded.1 P1 P2 Upper_Bounds M1 80 25 200 M2 20 75 200 We can formulate the above problem as the following linear program. \\[\\begin{align*} \\mbox{maximize: } &amp;&amp; 25x + 30y &amp; \\\\ \\mbox{subject to: } &amp;&amp; 80x + 20y &amp; \\le 200 \\\\ &amp;&amp; 25x + 75y &amp; \\le 200 \\\\ &amp;&amp; x, y &amp; \\ge 0 \\end{align*}\\] Here \\(x\\), \\(y\\) are the units of \\(P_1\\) and \\(P_2\\) produced each day. One way to attempt this problem is to assume that we’ll use both the machines as much as possible i.e. that the two inequalities are in fact equalities. We can see then that the common solution to the two is \\((x_1, x_2) = (2, 2)\\). For this solution the total profit is \\(110\\) units. Exercise 1.1 How to we know that this is the maximum profit possible? Is it possible to increase the profit by not using either of the two machines at their full capacity? Exercise 1.2 Consider the setup in Example 1.1 again. Suppose in addition, you can only store a total of 3 units of \\(P_1\\) and \\(P_2\\) combined each day so your production quantity should not exceed that amount. What is the new LP and the new optimal solution? Note that the solution \\((x, y) = (2, 2)\\) is no longer possible as \\(x + y &gt; 3\\). So, we cannot use both the machines to their maximum capacity. "],["standard-form-of-a-linear-program.html", "Standard form of a linear program Exercises", " Standard form of a linear program It is inconvenient to work with a linear program in the form described in Equation (1.1) because each of the three kinds of constraints \\(\\leq\\), \\(=\\), and \\(\\geq\\) require different manipulations. A linear program of the following form is said to be in a standard form: \\[\\begin{equation} \\begin{aligned} \\mbox{maximize: } &amp;&amp; c_1 x_1 + \\dots + c_n x_n &amp; \\\\ \\mbox{subject to: } &amp;&amp; a_{11} x_1 + \\dots + a_{1n} x_n &amp; \\leq b_1 \\\\ &amp;&amp; a_{21} x_1 + \\dots + a_{2n} x_n &amp; \\leq b_2 \\\\ &amp;&amp; \\vdots &amp; \\\\ &amp;&amp; a_{m1} x_1 + \\dots + a_{mn} x_n &amp; \\leq b_m \\\\ &amp;&amp; x_1, \\dots, x_n &amp; \\geq 0. \\end{aligned} \\tag{1.2} \\end{equation}\\] The last line is a succinct way of writing the \\(n\\) different inequalities \\(x_j \\geq 0\\) for \\(j = 1, \\dots, n\\). A linear program in a standard form can be written more succinctly using vectors and matrices as follows. \\[\\begin{equation} \\begin{aligned} \\mbox{maximize: } &amp;&amp; c^T x &amp; \\\\ \\mbox{subject to: } &amp;&amp; A x &amp; \\leq b \\\\ &amp;&amp; x &amp; \\geq 0. \\end{aligned} \\tag{1.3} \\end{equation}\\] where now \\(x\\) is the vector of decision variables, \\(c\\) and \\(b\\) are vectors of real numbers, and \\(A\\) is the matrix of constraint coefficients. This enables us to use tools from Linear Algebra to solve LP problems. 1.1.1 Converting LP to standard form Notice that maximizing the objective function \\(\\zeta\\) is equivalent to minimizing the negative of the objective function \\(-\\zeta\\). From now on, we will assume that we are always trying to maximize the objective function. Two linear programs LP and LP’ are said to be equivalent if for any feasible solution \\((x_1, \\dots, x_n)\\) to LP, there exists a feasible solution \\((x&#39;_1, x&#39;_2, \\dots, x&#39;_{n&#39;})\\) to LP’ such that \\(\\zeta(x_1, \\dots, x_n) = \\zeta&#39;(x&#39;_1, x&#39;_2, \\dots, x&#39;_{n&#39;})\\) and vice versa. Thus solving LP is equivalent to LP’. Remark. LP and LP’ can have a different number of decision variables i.e. it is not necessarily true that \\(n = n&#39;.\\) We do not require a one-to-one correspondence between the solutions of LP and LP’ i.e. for a feasible solution \\((x_1, \\dots, x_n)\\) to LP there could potentially be multiple feasible solutions \\((x_1&#39;, \\dots, x_n&#39;)\\) to LP’ satisfying \\(\\zeta(x_1, \\dots, x_n) = \\zeta&#39;(x&#39;_1, x&#39;_2, \\dots, x&#39;_{n&#39;})\\). Similarly, in the other direction. Theorem 1.1 Every linear program is equivalent to a linear program in the standard form. Proof. The proof is by explicit construction. Consider the LP in (1.1). If LP is in the standard form, then we’re done. If not, then one of the following conditions about the constraints must be true: One of the linear constraints is an upper bound and has the form \\[a_{i1} x_1 + \\dots + a_{in} x_n \\geq b_i.\\] One of the linear constraints is an equality and has the form \\[a_{i1} x_1 + \\dots + a_{in} x_n = b_i.\\] One of the variables has a “negativity constraint” \\(x_j \\leq 0\\). A “positivity constraint” \\(x_j \\geq 0\\) is missing. Note that we’re already assuming that the objective function is “fixed” i.e. that our goal is to maximize \\(\\zeta\\). We “fix” each of the above errors sequentially: Case 1: We multiply both sides by \\(-1\\) giving us the desired inequality \\[-a_{i1} x_1 + \\dots + -a_{in} x_n \\leq -b_i.\\] Case 2: We replace \\(a_{i1} x_1 + \\dots + a_{in} x_n = b_i\\) with two inequalities \\[\\begin{align*} a_{i1} x_1 + \\dots + a_{in} x_n &amp;\\leq b_i \\\\ a_{i1} x_1 + \\dots + a_{in} x_n &amp;\\geq b_i \\end{align*}\\] which reduces the problem to Case 1. Case 3: We simply replace \\(x_j\\) with \\(-x_j\\) everywhere. Case 4: We let \\(x_j = x_j&#39; - x_j&#39;&#39;\\) for two new decision variables \\(x_j&#39;\\) and \\(x_j&#39;&#39;\\) where \\(x_j&#39;, x_j&#39;&#39; \\geq 0\\). We can do this because any real number can written as a difference of two positive real numbers. One can show that in each step the modified LP is equivalent to the original LP (Exercise 1.4). Exercises Exercise 1.3 Prove that the equivalence of linear programs is an equivalence relation. Exercise 1.4 Prove that the algorithm in the proof of Theorem 1.1 produces a linear program that is equivalent to the original linear program. "],["simplex-method.html", "Simplex Method", " Simplex Method In a first course in Linear Algebra, you learn to solve equations of the form \\(A x = b\\). In LP, we are increasing the complexity on two levels. We are replacing \\(Ax = b\\) with \\(Ax \\leq b\\) and \\(x \\ge 0\\). Solving these equations will give us the set of all feasible solutions. Among all the feasible solutions we then need to find an optimal feasible solution. As it turns out, unlike in the Linear Algebra case, where the solutions to \\(Ax = b\\) forms an affine space of a finite dimension and can be described by a basis, it is not easy to describe the solutions to \\(Ax \\leq b\\) and \\(x \\ge 0\\) succinctly. So, we abandon the quest for finding all feasible solutions, for now. Instead, we will directly try to find optimal feasible solutions. The strategy for solving LP, which is mostly brute force, is called the simplex method. Implementation details of the simplex method vary a lot, but the general idea is the following. Guess a feasible solution. Check if there is a direction along which the objective value increases while staying feasible. If yes, then perturb the guess as much as possible in that direction and repeat. If no, then we have found a local maxima. Because all our functions are linear, a local maxima will also be a global maxima and the solution will be a feasible solution. Exercise 1.5 What would be some of the difficulties in running the above algorithm? Problem from https://ocw.mit.edu/courses/sloan-school-of-management/15-053-optimization-methods-in-management-science-spring-2013/tutorials/MIT15_053S13_tut01.pdf↩︎ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
