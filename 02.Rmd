# Geometry, Equivalence, Standard form 

## Using geometry to solve linear programs
Here is one way to see that $(2, 2)$ is the optimal solution to Example \@ref(resource-allocation-problem). For any constant $c$, the equation 
\begin{equation}
  c = 25x + 30y
  (\#eq:objective-function)
\end{equation}
describes a line in $\mathbb{R}^2$. The points below this line have an objective value less than $c$ and the points above this line have an objective value greater than $c$. To see that $(2, 2)$ is the optimal solution, we simply need to see that the line of the form \@ref(eq:objective-function) that passes through it, namely $110 = 25x + 30y$, lies above the feasible region as seen in the following figure.

```{r, fig-resource-allocation-line, echo=FALSE, warning=FALSE, fig.cap="The feasible region lies below the line $110 = 25x + 30y$ and intersects it at $(2, 2)$."}

library(ggplot2)

resource_allocation_plot + 
  geom_point(aes(x=2, y=2), color = "black") +
  geom_text(aes(x=2, y=2), label="(2, 2)", vjust=-.5, hjust=0) +
  stat_function(fun = function(x) {(110 - 25*x)/30}, color = "black")
```

:::{.exercise}
Find an objective function $\zeta$ for which the point $(2, 2)$ is no longer an optimal solution.

:::

We can use the same reasoning to solve Exercise \@ref(exr:resource-allocation-problem-extra), which is modelled by the following linear program.

```{r, latex-resource-allocation-extra, results = 'asis', echo = FALSE}
ex_1_1_lp <- new_linear_program(
  obj = c(25, 30),
  A = matrix(c(80, 20, 25, 75, 1, 1), nrow = 2, byrow = TRUE),
  b = c(200, 200, 3),
  variables = c('x', 'y')
)

cat(str_math(ex_1_1_lp))
```

The line of the form \@ref(eq:objective-function) for which the entire feasible region lies below it and which intersects the feasible region is $87.5 = 25x + 30y$ and it passes through $(0.5, 2.5)$. Here, instead of maximizing the time spend on each machine, we're maximizing the amount of resources produced and the time spent on the second machine $M_2$. As a result, our profit dropped from $110$ to $87.5$.

```{r, fig-resource-allocation-extra-line, echo=FALSE, warning=FALSE, fig.cap = "The feasible region lies below the line $87.5 = 25x + 30y$ and intersects it at $(0.5, 2.5)$."}

library(ggplot2)

resource_allocation_extra_plot <- resource_allocation_plot + geom_polygon(
      data = data.frame(
        x = c(0, 0, 3),
        y = c(0, 3, 0)),
      aes(x = x, y = y, fill = "3: x + y <= 3"),
      inherit.aes = FALSE, alpha = 0.4)

resource_allocation_extra_plot +
  stat_function(fun = function(x) {((0.5 * 25 + 2.5 * 30) - 25*x)/30}, color = "black") +
  geom_point(aes(x=0.5, y=2.5), color = "black") +
  geom_text(aes(x=0.5, y=2.5), label="(0.5, 2.5)", vjust=-.5, hjust=0)
```

In both the problems above, we found the line *by inspection*. There is no efficient way to do this purely algebraically! As a consequence, there is no way to create a useful algorithm out of this method. We'll see a completely new way of approaching this problem using the simplex method. Instead of trying to find the optimal hyperplane, the simplex method jumps from vertex to vertex searching for the optimal solution. The existence of the optimal solution at a vertex is guaranteed by the following theorem.

::: {.theorem #fundamental-theorem-of-LP}
Suppose the solution set of a linear program in two variables is non-empty and bounded. Then, 

1. The feasible set is a [convex polygon](https://en.wikipedia.org/wiki/Convex_polygon).
2. The optimal value is attained at a vertex of the feasible set.

:::
This theorem also holds in more than two variables, in which case instead of a convex polygon we get a [convex polytope](https://en.wikipedia.org/wiki/Convex_polytope). The proof is in the following exercises.

::: {.exercise #proof-of-fundamental-theorem-of-LP}
Consider the following linear program 

\begin{align*}
\mbox{maximize: } && c_1 x + c_2 y & =: \zeta(x,y) \\
\mbox{subject to: } 
  && a_{11} x + a_{12} y & \lesseqqgtr b_1 \\
  && a_{21} x + a_{22} y & \lesseqqgtr b_2 \\
  && \vdots &  \\
  && a_{m1} x + a_{m2} y & \lesseqqgtr b_m. 
\end{align*}

Let $\mathcal{S}$ denote the feasible set. *Assume that $\mathcal{S}$ is bounded and non-empty.*   Let $P = (x_1, y_1)$ and $Q = (x_2, y_2)$ be two distinct points in $\mathbb{R}^2$. Let $R = (x_3, y_3)$ be a point lying on the line segment between $P$ and $Q$. 

1. Show that $R = tP + (1-t)Q$ for some $t$ in $(0, 1)$.
2. Show that if that both $P$ and $Q$ satisfy the constraints above, then $R$ also satisfies the constraints. 
   Conclude that if $P$ and $Q$ are in $\mathcal{S}$ then so is $R$.

  A subset of $\mathbb{R}^n$ is called **convex** if, given any two points in the subset, the subset contains the whole line segment that joins them. The above exercises show that $\mathcal{S}$ is a convex subset of $\mathbb{R}^2$. Furthermore, because it is bounded and defined using linear equations, it is a \href{https://en.wikipedia.org/wiki/Convex_polygon}{convex polygon}.

3. Show that $\zeta(R) = t\zeta(P) + (1-t)\zeta(Q)$ for some $t$ in $ (0,1) $.    
4. Show that either $\zeta(R) \le \zeta(P)$ or $\zeta(R) \le \zeta(Q)$ (or both).
5. Let $R'$ be a point in the interior of $\mathcal{S}$. Argue that there is a point $P'$ on the boundary of $\mathcal{S}$ such that $\zeta(R') \le \zeta(P')$.
6. Let $R''$ be a point in the interior of one of the edges of $\mathcal{S}$. Argue that there is a vertex $P''$ of $\mathcal{S}$ such that $\zeta(R'') \le \zeta(P'')$. Conclude that $\zeta$ attains its maximum value at a vertex of $\mathcal{S}$.

:::  

## Equivalence of linear programs 

Before we get to the simplex method, we'll need to *standardize* our linear programs. 

The first step is to notice that every linear program can be changed to a *maximization problem* as minimizing a function $\zeta$ is the same as maximizing the function $-\zeta$.

> **From now on, we'll assume that the goal of our linear programs is to _maximize_ the objective function.**

Two (maximizing) linear programs LP and LP' are said to be *equivalent* if for any feasible solution $(x_1, \dots, x_n)$ to LP, there exists a feasible solution $(x'_1, x'_2, \dots, x'_{n'})$ to LP' with the same objective value \[ \zeta(x_1, \dots, x_n) = \zeta'(x'_1, x'_2, \dots, x'_{n'}), \] and vice versa. Thus solving LP is equivalent to LP'.

:::{.remark #lp-equiv-definition}

&nbsp;

1. LP and LP' can have a different number of decision variables i.e. we do not require $n = n'.$
2. There need not be a one-to-one correspondence between the feasible sets of LP and LP' i.e. for a feasible solution to LP there could be multiple feasible solutions with the same objective value. Similarly, in the other direction.

:::

:::{.remark}

Even though equivalence of linear programs only requires the existence of an abstract correspondence between the feasible sets of LP and LP', in practice, one constructs linear transformations $T: \mathbb{R}^n \to \mathbb{R}^{n'}$ and $S: \mathbb{R}^{n'} \to \mathbb{R}^{n}$ which map the feasible set of LP to LP' and the feasible set of LP' to LP, respectively. These linear transformations need not be inverses of each other, or even isomorphisms. They only need to preserve the objective values.

:::

::: {.example}
The following linear programs are all equivalent to the linear program in Example \@ref(resource-allocation-problem).

```{r, equiv-lp-1, results = 'asis', echo = FALSE}
equiv_lp_1 <- new_linear_program(
  obj = c(25, 30),
  A = matrix(c(800, 200, 25, 75), nrow = 2, byrow = TRUE),
  b = c(2000, 200),
  variables = c('x', 'y')
)

equiv_lp_2 <- new_linear_program(
  obj = c(250, 300),
  A = matrix(c(80, 20, 25, 75), nrow = 2, byrow = TRUE),
  b = c(20, 20),
  variables = c('x', 'y')
)

equiv_lp_3 <- new_linear_program(
  obj = c(25, 30),
  A = matrix(c(80, 20, 25, 75, 2, 2), nrow = 2, byrow = TRUE),
  b = c(200, 200, 200),
  variables = c('x', 'y')
)

cat(str_math(equiv_lp_2))
cat(str_math(equiv_lp_1))
cat(str_math(equiv_lp_3))
```

:::

::: {.example}
The following two linear programs are equivalent to each other
\begin{align*}
\begin{aligned}
\mbox{maximize: } && x + y & \\
\mbox{subject to: } && 0 \le x &\le 1 \\
 && 0 \le y &\le 1 
\end{aligned}
&&
\begin{aligned}
\mbox{maximize: } && z & \\
\mbox{subject to: } && 0 \le z &\le 1
\end{aligned}
\end{align*}
via the transformations $T(x, y) = x + y$ and $S(z) = (z/2, z/2)$.

:::


## Standard form of linear programs

A linear program of the following form is said to be in a *standard form*:
\begin{equation}
  \begin{array}{lrrrrrrrrr}
    \mbox{maximize: }  & c_1 x_1 & + & \dots & + & c_n x_n & \\
    \mbox{subject to: } 
      & a_{11} x_1 & + & \dots & + & a_{1n} x_n & \leq & b_1 \\
      & a_{21} x_1 & + & \dots & + & a_{2n} x_n & \leq & b_2 \\
      & \vdots &  \\
      & a_{m1} x_1 & + & \dots & + & a_{mn} x_n & \leq & b_m 
  \end{array} \\
      x_1, \: \dots \: , \: x_{n} \: \geq \: 0
  (\#eq:standard-lp)
\end{equation}
Such a linear program can be written more succinctly using vectors and matrices as follows.
\begin{equation}
  \begin{aligned}
    \mbox{maximize: } && c^T x & \\
    \mbox{subject to: } 
      && A x & \leq b \\
      && x & \geq 0.
  \end{aligned}
  (\#eq:standard-lp-matrix-form)
\end{equation}
where $x$ is now the vector of decision variables, $c$ and $b$ are vectors of real numbers, and $A$ is the matrix of constraint coefficients. This enables us to use tools from Linear Algebra to solve linear programs.

::: {.theorem #lp-to-standard-form}
Every linear program is equivalent to a linear program in the standard form.
:::

::: {.proof}
The proof is by an explicit algorithm. Consider the linear program in \@ref(eq:intro-lp), where we're assuming that the goal is to maximize the objective function. If it is in the standard form, then we're done. If not, then there must be a finite number of *errors* of the following types:

1. A linear constraint is an upper bound and has the form $$a_{i1} x_1 + \dots + a_{in} x_n \geq b_i.$$
2. A linear constraint is an equality and has the form $$a_{i1} x_1 + \dots + a_{in} x_n = b_i.$$
3. A variable $x_j$ has a *negativity constraint* $x_j \leq 0$.
4. A variable $x_j$ is missing a *sign constraint*.

We *fix each error* sequentially while making sure that no new errors are introduced, thereby ensuring termination of the algorithm.

1. We replace the constraint with \[ -a_{i1} x_1 + \dots + -a_{in} x_n \leq -b_i. \]
2. We replace the constraint with two constraints
  \begin{align*}
  a_{i1} x_1 + \dots + a_{in} x_n &\leq b_i \\
  -a_{i1} x_1 - \dots - a_{in} x_n &\leq -b_i,
  \end{align*}
3. We let $y_j = -x_j$ and create a new linear program using the variables $x_1$, $\dots$, $x_{j-1}$, $y_j$, $x_{j+1}$, $\dots$, $x_n$ by replacing $x_j$ with $-y_j$ everywhere.
4. We let $x_j = y_j - z_j$ for two new decision variables $y_j$ and $z_j$ with $y_j, z_j \geq 0$ and create a new linear program using the variables $x_1$, $\dots$, $x_{j-1}$, $y_j$, $z_j$, $x_{j+1}$, $\dots$, $x_n$ by replacing $x_j$ with $y_j - z_j$ everywhere. We can do this because any real number can written as a difference of two positive real numbers.

One can show that in each step the modified LP is equivalent to the original LP.
:::

:::{.exercise #lp-to-standard-form}
Prove that the algorithm in the proof of Theorem \@ref(thm:lp-to-standard-form) produces a linear program that is equivalent to the original linear program.
:::
